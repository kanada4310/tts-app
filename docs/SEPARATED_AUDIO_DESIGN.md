# éŸ³å£°åˆ†å‰²æ–¹å¼ - è©³ç´°è¨­è¨ˆæ›¸

**ä½œæˆæ—¥**: 2025-10-23
**ç›®çš„**: ãƒªãƒ”ãƒ¼ãƒˆæ©Ÿèƒ½ã®ãƒã‚°ã‚’å®Œå…¨è§£æ±ºã™ã‚‹ãŸã‚ã€éŸ³å£°ã‚’æ–‡ã”ã¨ã«åˆ†å‰²ã™ã‚‹æ–¹å¼ã‚’å°å…¥

---

## ğŸ“Š æ¦‚è¦

### ç¾åœ¨ã®æ–¹å¼ï¼ˆçµåˆæ–¹å¼ï¼‰

```
ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰: æ–‡1 + æ–‡2 + æ–‡3 â†’ 1ã¤ã®mp3ãƒ•ã‚¡ã‚¤ãƒ«
ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰: <audio src="combined.mp3"> + ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã§åˆ¶å¾¡
å•é¡Œç‚¹: ã‚·ãƒ¼ã‚¯æ“ä½œã®ã‚¿ã‚¤ãƒŸãƒ³ã‚°èª¤å·®ã€ãƒãƒ¼ã‚ºå‰ã®éŸ³è¢«ã‚Š
```

### æ–°ã—ã„æ–¹å¼ï¼ˆåˆ†å‰²æ–¹å¼ï¼‰

```
ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰: æ–‡1 â†’ mp3_1, æ–‡2 â†’ mp3_2, æ–‡3 â†’ mp3_3
ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰: <audio src="mp3_X"> ã‚’æ–‡ã”ã¨ã«åˆ‡ã‚Šæ›¿ãˆ
ãƒ¡ãƒªãƒƒãƒˆ: ã‚¿ã‚¤ãƒŸãƒ³ã‚°èª¤å·®ã‚¼ãƒ­ã€ãƒªãƒ”ãƒ¼ãƒˆãŒå˜ç´”ãªå†ç”Ÿæ“ä½œ
```

---

## ğŸ¯ è¨­è¨ˆæ–¹é‡

### ã‚³ã‚¢åŸå‰‡

1. **æ—¢å­˜APIã¨ã®äº’æ›æ€§ç¶­æŒ**
   - `/api/tts-with-timings`ï¼ˆçµåˆæ–¹å¼ï¼‰ã¯æ®‹ã™
   - `/api/tts-with-timings-separated`ï¼ˆåˆ†å‰²æ–¹å¼ï¼‰ã‚’æ–°è¦è¿½åŠ 
   - ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã§åˆ‡ã‚Šæ›¿ãˆå¯èƒ½ã«ã™ã‚‹

2. **APIåˆ©ç”¨æ–™ã®å¢—åŠ ãªã—**
   - æ—¢å­˜ã®`generate_speech_with_timings()`ã¯æ–‡ã”ã¨ã«ç”Ÿæˆæ¸ˆã¿
   - çµåˆå‡¦ç†ã‚’çœç•¥ã™ã‚‹ã ã‘ â†’ ã‚³ã‚¹ãƒˆå¤‰ã‚ã‚‰ãš

3. **æ®µéšçš„ç§»è¡Œ**
   - ã¾ãšåˆ†å‰²æ–¹å¼ã‚’å®Ÿè£…
   - å‹•ä½œç¢ºèªå¾Œã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚’åˆ†å‰²æ–¹å¼ã«å¤‰æ›´
   - å•é¡ŒãŒã‚ã‚Œã°çµåˆæ–¹å¼ã«æˆ»ã›ã‚‹

---

## ğŸ”§ å®Ÿè£…è©³ç´°

### Phase 1: ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰å®Ÿè£…ï¼ˆ30-45åˆ†ï¼‰

#### 1.1 æ–°ã—ã„ã‚µãƒ¼ãƒ“ã‚¹é–¢æ•°ã®è¿½åŠ 

**ãƒ•ã‚¡ã‚¤ãƒ«**: `backend/app/services/openai_service.py`

**æ–°è¦é–¢æ•°**: `generate_speech_separated()`

```python
from typing import List, Tuple
import base64

def generate_speech_separated(
    self,
    sentences: List[str],
    voice: str = "nova",
    format: str = "mp3"  # mp3ã«å›ºå®šï¼ˆffmpegäº’æ›æ€§ï¼‰
) -> Tuple[List[dict], float]:
    """
    Generate speech with separated audio files per sentence

    Args:
        sentences: List of sentences to convert
        voice: Voice to use
        format: Audio format (mp3 recommended for compatibility)

    Returns:
        Tuple of (audio_segments, total_duration)

        audio_segments: List of dicts with:
            - index: int (sentence index)
            - audio_base64: str (base64-encoded audio)
            - text: str (sentence text)
            - duration: float (audio duration in seconds)
    """
    try:
        from pydub import AudioSegment

        # Validate inputs (same as existing function)
        valid_voices = ["alloy", "echo", "fable", "onyx", "nova", "shimmer"]
        if voice not in valid_voices:
            raise TTSGenerationError(
                f"Invalid voice: {voice}. Must be one of {valid_voices}",
                error_code=ERROR_TTS_FAILED
            )

        valid_formats = ["opus", "mp3", "aac", "flac"]
        if format not in valid_formats:
            raise TTSGenerationError(
                f"Invalid format: {format}. Must be one of {valid_formats}",
                error_code=ERROR_TTS_FAILED
            )

        if not sentences or len(sentences) == 0:
            raise TTSGenerationError(
                "No sentences provided",
                error_code=ERROR_TTS_FAILED
            )

        # Generate TTS for each sentence (same as existing code)
        audio_segments = []
        total_duration = 0.0

        for idx, sentence_text in enumerate(sentences):
            if not sentence_text.strip():
                continue

            # Generate TTS for this sentence
            response = self.client.audio.speech.create(
                model=OPENAI_TTS_MODEL,
                voice=voice,
                input=sentence_text,
                response_format=format,
                speed=OPENAI_TTS_SPEED
            )

            # Read audio data
            audio_data = BytesIO()
            for chunk in response.iter_bytes():
                audio_data.write(chunk)
            audio_bytes = audio_data.getvalue()

            # Get precise duration
            duration = self._get_audio_duration(audio_bytes, format)

            # Encode to base64
            audio_base64 = base64.b64encode(audio_bytes).decode('utf-8')

            # Store segment
            audio_segments.append({
                "index": idx,
                "audio_base64": audio_base64,
                "text": sentence_text,
                "duration": duration
            })

            total_duration += duration

            print(f"[TTS Separated] Sentence {idx}: {duration:.3f}s, text: {sentence_text[:30]}...")

        print(f"[TTS Separated] Generated {len(audio_segments)} separate audio files, total: {total_duration:.3f}s")

        return audio_segments, total_duration

    except TTSGenerationError:
        raise
    except Exception as e:
        raise TTSGenerationError(
            f"OpenAI TTS separated generation failed: {str(e)}",
            error_code=ERROR_TTS_FAILED
        ) from e
```

**æ—¢å­˜é–¢æ•°ã¨ã®æ¯”è¼ƒ**:
- `generate_speech_with_timings()`: 286-291è¡Œã§çµåˆã€317è¡Œã§export
- `generate_speech_separated()`: **çµåˆå‡¦ç†ã‚’å‰Šé™¤ã€base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã®ã¿**

#### 1.2 æ–°ã—ã„ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã®è¿½åŠ 

**ãƒ•ã‚¡ã‚¤ãƒ«**: `backend/app/api/routes/tts.py`

**æ–°è¦ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ**: `/api/tts-with-timings-separated`

```python
@router.post(
    "/tts-with-timings-separated",
    response_model=None,
    responses={
        200: {
            "content": {"application/json": {}},
            "description": "JSON with separated audio segments"
        },
        400: {"model": TTSErrorResponse},
        422: {"model": TTSErrorResponse},
        429: {"model": TTSErrorResponse},
        500: {"model": TTSErrorResponse},
    }
)
@limiter.limit("100/hour")
async def generate_speech_separated(
    request: Request,
    tts_request: TTSRequest
):
    """
    Generate speech with separated audio files per sentence

    Returns:
        JSON with array of audio segments (each sentence = separate audio file)
    """
    try:
        # Validate sentences
        if not tts_request.sentences or len(tts_request.sentences) == 0:
            raise HTTPException(
                status_code=400,
                detail={
                    "error": "MISSING_SENTENCES",
                    "message": "Sentences array is required for this endpoint"
                }
            )

        print(f"TTS separated - Sentences: {len(tts_request.sentences)}, Voice: {tts_request.voice}")

        # Generate separated audio files
        audio_segments, total_duration = openai_service.generate_speech_separated(
            sentences=tts_request.sentences,
            voice=tts_request.voice,
            format=tts_request.format
        )

        # Return JSON response
        return JSONResponse(content={
            "audio_segments": audio_segments,
            "total_duration": total_duration,
            "format": tts_request.format
        })

    except TTSGenerationError as e:
        import traceback
        print(f"TTS Generation Error: {e.message}")
        print(traceback.format_exc())
        raise HTTPException(
            status_code=400 if "Invalid" in e.message else 500,
            detail={
                "error": e.error_code,
                "message": e.message
            }
        )
    except HTTPException:
        raise
    except Exception as e:
        import traceback
        print(f"Unexpected TTS Error: {str(e)}")
        print(traceback.format_exc())
        raise HTTPException(
            status_code=500,
            detail={
                "error": ERROR_INTERNAL,
                "message": f"Internal server error: {str(e)}"
            }
        )
```

#### 1.3 ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚¹ã‚­ãƒ¼ãƒã®è¿½åŠ ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰

**ãƒ•ã‚¡ã‚¤ãƒ«**: `backend/app/schemas/tts.py`

```python
class AudioSegment(BaseModel):
    """Single audio segment for one sentence"""
    index: int = Field(..., description="Sentence index")
    audio_base64: str = Field(..., description="Base64-encoded audio data")
    text: str = Field(..., description="Sentence text")
    duration: float = Field(..., description="Audio duration in seconds")


class TTSResponseSeparated(BaseModel):
    """Response for separated audio generation"""
    audio_segments: List[AudioSegment] = Field(
        ...,
        description="Array of audio segments (one per sentence)"
    )
    total_duration: float = Field(
        ...,
        description="Total duration of all segments"
    )
    format: str = Field(
        ...,
        description="Audio format used"
    )
```

---

### Phase 2: ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰å®Ÿè£…ï¼ˆ2.5-3æ™‚é–“ï¼‰

#### 2.1 APIé–¢æ•°ã®è¿½åŠ 

**ãƒ•ã‚¡ã‚¤ãƒ«**: `frontend/src/services/api/tts.ts`

```typescript
/**
 * TTS with separated audio files (one per sentence)
 */
export async function performTTSSeparated(
  text: string,
  sentences: string[],
  voice: string = 'nova',
  format: string = 'mp3'
): Promise<{
  audioBlobs: Blob[]
  durations: number[]
  totalDuration: number
}> {
  const request: TTSRequest = {
    text,
    voice,
    format,
    sentences,
  }

  // Call new endpoint
  const response = await apiPost<{
    audio_segments: Array<{
      index: number
      audio_base64: string
      text: string
      duration: number
    }>
    total_duration: number
    format: string
  }>(API_ENDPOINTS.TTS_WITH_TIMINGS_SEPARATED, request)

  // Decode each audio segment
  const audioBlobs: Blob[] = []
  const durations: number[] = []

  for (const segment of response.audio_segments) {
    // Decode base64
    const audioData = atob(segment.audio_base64)
    const audioArray = new Uint8Array(audioData.length)
    for (let i = 0; i < audioData.length; i++) {
      audioArray[i] = audioData.charCodeAt(i)
    }

    const mimeType = format === 'mp3' ? 'audio/mpeg' : `audio/${format}`
    const audioBlob = new Blob([audioArray], { type: mimeType })

    audioBlobs.push(audioBlob)
    durations.push(segment.duration)
  }

  return {
    audioBlobs,
    durations,
    totalDuration: response.total_duration,
  }
}
```

**å®šæ•°è¿½åŠ **: `frontend/src/constants/api.ts`

```typescript
export const API_ENDPOINTS = {
  // ... existing endpoints
  TTS_WITH_TIMINGS_SEPARATED: '/api/tts-with-timings-separated',
} as const
```

#### 2.2 AudioPlayer ã®å¤§å¹…æ”¹é€ 

**ãƒ•ã‚¡ã‚¤ãƒ«**: `frontend/src/components/features/AudioPlayer/AudioPlayer.tsx`

**æ–°ã—ã„State**:

```typescript
// Separated audio mode
const [audioSegments, setAudioSegments] = useState<string[]>([]) // Array of blob URLs
const [segmentDurations, setSegmentDurations] = useState<number[]>([])
const [currentSegmentIndex, setCurrentSegmentIndex] = useState<number>(0)
const [useSeparatedMode, setUseSeparatedMode] = useState<boolean>(true) // Toggle between modes
```

**Propsè¿½åŠ **:

```typescript
export interface AudioPlayerProps {
  // ... existing props
  audioSegments?: Blob[] // New: separated audio blobs
  segmentDurations?: number[] // New: duration of each segment
}
```

**åˆæœŸåŒ–å‡¦ç†**:

```typescript
// Initialize audio segments
useEffect(() => {
  if (audioSegments && audioSegments.length > 0) {
    console.log('[AudioPlayer] Using separated audio mode:', audioSegments.length, 'segments')

    // Create blob URLs
    const urls = audioSegments.map(blob => URL.createObjectURL(blob))
    setAudioSegments(urls)

    // Store durations
    if (segmentDurations) {
      setSegmentDurations(segmentDurations)
    }

    // Load first segment
    if (audioRef.current && urls[0]) {
      audioRef.current.src = urls[0]
      audioRef.current.load()
    }

    // Cleanup on unmount
    return () => {
      urls.forEach(url => URL.revokeObjectURL(url))
    }
  }
}, [audioSegments, segmentDurations])
```

**æ–‡çµ‚äº†æ™‚ã®åˆ‡ã‚Šæ›¿ãˆãƒ­ã‚¸ãƒƒã‚¯**:

```typescript
const handleSegmentEnded = async () => {
  console.log('[AudioPlayer] Segment ended:', currentSegmentIndex)

  // Check repeat logic
  const newRepeatCount = currentRepeat + 1

  if (repeatCount === -1 || newRepeatCount < repeatCount) {
    // Repeat current segment
    console.log(`[AudioPlayer] Repeating segment ${currentSegmentIndex}: ${newRepeatCount}/${repeatCount}`)
    setCurrentRepeat(newRepeatCount)

    if (audioRef.current) {
      audioRef.current.currentTime = 0
      await audioRef.current.play()
    }
  } else {
    // Move to next segment
    setCurrentRepeat(0)

    if (autoAdvance && currentSegmentIndex < audioSegments.length - 1) {
      // Load next segment
      const nextIndex = currentSegmentIndex + 1
      console.log(`[AudioPlayer] Advancing to segment ${nextIndex}`)

      setCurrentSegmentIndex(nextIndex)
      onSentenceChange?.(nextIndex)

      if (audioRef.current && audioSegments[nextIndex]) {
        audioRef.current.src = audioSegments[nextIndex]
        audioRef.current.load()

        // Apply pause if enabled
        if (pauseConfig.enabled) {
          setIsPauseBetweenSentences(true)
          setTimeout(() => {
            setIsPauseBetweenSentences(false)
            audioRef.current?.play()
          }, pauseConfig.duration * 1000)
        } else {
          await audioRef.current.play()
        }
      }
    } else {
      // End of all segments
      console.log('[AudioPlayer] All segments completed')
      setIsPlaying(false)
      onPlayStateChange?.(false)
      onPlaybackComplete?.()
    }
  }
}

// Replace handleAudioEnded
useEffect(() => {
  if (!audioRef.current) return

  const audio = audioRef.current
  audio.addEventListener('ended', handleSegmentEnded)

  return () => {
    audio.removeEventListener('ended', handleSegmentEnded)
  }
}, [currentSegmentIndex, repeatCount, currentRepeat, autoAdvance, pauseConfig])
```

**æ–‡ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆå‰/æ¬¡ï¼‰**:

```typescript
const handlePrevSentence = () => {
  if (currentSegmentIndex > 0) {
    const prevIndex = currentSegmentIndex - 1
    console.log(`[AudioPlayer] Moving to previous segment ${prevIndex}`)

    setCurrentSegmentIndex(prevIndex)
    setCurrentRepeat(0)
    onSentenceChange?.(prevIndex)

    if (audioRef.current && audioSegments[prevIndex]) {
      audioRef.current.src = audioSegments[prevIndex]
      audioRef.current.load()
      audioRef.current.play()
    }
  }
}

const handleNextSentence = () => {
  if (currentSegmentIndex < audioSegments.length - 1) {
    const nextIndex = currentSegmentIndex + 1
    console.log(`[AudioPlayer] Moving to next segment ${nextIndex}`)

    setCurrentSegmentIndex(nextIndex)
    setCurrentRepeat(0)
    onSentenceChange?.(nextIndex)

    if (audioRef.current && audioSegments[nextIndex]) {
      audioRef.current.src = audioSegments[nextIndex]
      audioRef.current.load()
      audioRef.current.play()
    }
  }
}
```

**ã‚·ãƒ¼ã‚¯ãƒãƒ¼ã®ç°¡ç•¥åŒ–**:

```typescript
// Separated mode: ã‚·ãƒ¼ã‚¯ãƒãƒ¼ã¯ç¾åœ¨ã®æ–‡å†…ã®ã¿
// æ–‡å¢ƒç•Œãƒãƒ¼ã‚«ãƒ¼ã€ãƒ„ãƒ¼ãƒ«ãƒãƒƒãƒ—ã¯ä¸è¦ï¼ˆå„æ–‡ãŒç‹¬ç«‹ã—ã¦ã„ã‚‹ãŸã‚ï¼‰

const handleSeekbarClick = (e: React.MouseEvent<HTMLDivElement>) => {
  if (!audioRef.current || !progressBarRef.current) return

  const rect = progressBarRef.current.getBoundingClientRect()
  const clickX = e.clientX - rect.left
  const percentage = clickX / rect.width

  // Seek within current segment
  const segmentDuration = segmentDurations[currentSegmentIndex] || duration
  const newTime = percentage * segmentDuration

  audioRef.current.currentTime = newTime
}
```

#### 2.3 App.tsx ã®ä¿®æ­£

**ãƒ•ã‚¡ã‚¤ãƒ«**: `frontend/src/App.tsx`

```typescript
const [audioSegments, setAudioSegments] = useState<Blob[]>([])
const [segmentDurations, setSegmentDurations] = useState<number[]>([])

const handleGenerateAudio = async () => {
  // ... existing validation

  try {
    setIsGenerating(true)
    setError(null)

    // Use separated audio mode
    const { audioBlobs, durations, totalDuration } = await performTTSSeparated(
      ocrText,
      ocrSentences,
      'nova',
      'mp3'
    )

    setAudioSegments(audioBlobs)
    setSegmentDurations(durations)
    setAudioUrl('separated') // Flag to indicate separated mode

    console.log(`[App] Generated ${audioBlobs.length} audio segments, total: ${totalDuration}s`)
  } catch (err) {
    // ... error handling
  } finally {
    setIsGenerating(false)
  }
}

// Pass to AudioPlayer
<AudioPlayer
  audioUrl={audioUrl}
  audioSegments={audioSegments}
  segmentDurations={segmentDurations}
  sourceSentences={ocrSentences}
  // ... other props
/>
```

---

### Phase 3: ãƒ†ã‚¹ãƒˆè¨ˆç”»ï¼ˆ30-45åˆ†ï¼‰

#### 3.1 ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ãƒ†ã‚¹ãƒˆ

**æ‰‹å‹•ãƒ†ã‚¹ãƒˆ**:

```bash
# Start backend
cd backend
python -m uvicorn app.main:app --reload

# Test endpoint with curl
curl -X POST http://localhost:8000/api/tts-with-timings-separated \
  -H "Content-Type: application/json" \
  -d '{
    "text": "Hello. How are you? I am fine.",
    "sentences": ["Hello.", "How are you?", "I am fine."],
    "voice": "nova",
    "format": "mp3"
  }'
```

**æœŸå¾…ã•ã‚Œã‚‹ãƒ¬ã‚¹ãƒãƒ³ã‚¹**:

```json
{
  "audio_segments": [
    {
      "index": 0,
      "audio_base64": "base64_string...",
      "text": "Hello.",
      "duration": 0.5
    },
    {
      "index": 1,
      "audio_base64": "base64_string...",
      "text": "How are you?",
      "duration": 0.8
    },
    {
      "index": 2,
      "audio_base64": "base64_string...",
      "text": "I am fine.",
      "duration": 0.7
    }
  ],
  "total_duration": 2.0,
  "format": "mp3"
}
```

#### 3.2 ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ãƒ†ã‚¹ãƒˆ

**ãƒ†ã‚¹ãƒˆã‚·ãƒŠãƒªã‚ª**:

1. **åŸºæœ¬å†ç”Ÿ**
   - [ ] æ–‡0ã‹ã‚‰å†ç”Ÿé–‹å§‹
   - [ ] æ–‡0çµ‚äº†å¾Œã€è‡ªå‹•ã§æ–‡1ã¸ç§»å‹•
   - [ ] å…¨æ–‡å†ç”Ÿå¾Œã€åœæ­¢

2. **ãƒªãƒ”ãƒ¼ãƒˆæ©Ÿèƒ½**
   - [ ] ãƒªãƒ”ãƒ¼ãƒˆ3å›è¨­å®š â†’ æ–‡0ãŒ3å›å†ç”Ÿã•ã‚Œã‚‹
   - [ ] ãƒªãƒ”ãƒ¼ãƒˆã‚«ã‚¦ãƒ³ã‚¿ãƒ¼è¡¨ç¤ºï¼ˆ1/3, 2/3, 3/3ï¼‰
   - [ ] 3å›å†ç”Ÿå¾Œã€æ–‡1ã¸è‡ªå‹•ç§»å‹•

3. **æ–‡ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³**
   - [ ] ã€Œå‰ã®æ–‡ã€ãƒœã‚¿ãƒ³ â†’ æ–‡ãŒæˆ»ã‚‹
   - [ ] ã€Œæ¬¡ã®æ–‡ã€ãƒœã‚¿ãƒ³ â†’ æ–‡ãŒé€²ã‚€
   - [ ] æ–‡0ã§ã€Œå‰ã®æ–‡ã€æŠ¼ä¸‹ â†’ ä½•ã‚‚èµ·ã“ã‚‰ãªã„
   - [ ] æœ€çµ‚æ–‡ã§ã€Œæ¬¡ã®æ–‡ã€æŠ¼ä¸‹ â†’ ä½•ã‚‚èµ·ã“ã‚‰ãªã„

4. **ãƒãƒ¼ã‚ºæ©Ÿèƒ½**
   - [ ] ãƒãƒ¼ã‚º1ç§’è¨­å®š â†’ æ–‡çµ‚äº†å¾Œ1ç§’å¾…ã£ã¦ã‹ã‚‰æ¬¡ã®æ–‡
   - [ ] ãƒãƒ¼ã‚ºä¸­ã«ã€Œæ¬¡ã®æ–‡ã€ãƒœã‚¿ãƒ³ â†’ ã™ãã«æ¬¡ã®æ–‡ã¸

5. **é€Ÿåº¦èª¿æ•´**
   - [ ] 0.5x, 1.0x, 1.5x, 2.0x ã§å†ç”Ÿ
   - [ ] é€Ÿåº¦å¤‰æ›´å¾Œã‚‚æ–‡åˆ‡ã‚Šæ›¿ãˆãŒæ­£å¸¸

6. **æ–‡ãƒªã‚¹ãƒˆã¨ã®é€£æº**
   - [ ] æ–‡ãƒªã‚¹ãƒˆã§æ–‡3ã‚’ã‚¯ãƒªãƒƒã‚¯ â†’ æ–‡3ã‹ã‚‰å†ç”Ÿé–‹å§‹
   - [ ] å†ç”Ÿä¸­ã€ç¾åœ¨ã®æ–‡ãŒãƒã‚¤ãƒ©ã‚¤ãƒˆ

7. **ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°**
   - [ ] ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼æ™‚ã®ã‚¨ãƒ©ãƒ¼è¡¨ç¤º
   - [ ] éŸ³å£°èª­ã¿è¾¼ã¿å¤±æ•—æ™‚ã®å‡¦ç†

---

## ğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒ

### è»¢é€é‡ã®æ¯”è¼ƒ

**10æ–‡ã®ãƒ†ã‚­ã‚¹ãƒˆã®å ´åˆ**:

| æ–¹å¼ | ãƒ•ã‚¡ã‚¤ãƒ«æ•° | ç·ã‚µã‚¤ã‚ºï¼ˆæ¨å®šï¼‰ | åˆå›èª­ã¿è¾¼ã¿ | å‚™è€ƒ |
|------|----------|----------------|------------|------|
| çµåˆæ–¹å¼ | 1 | 50KB | 50KB | 1ã¤ã®å¤§ããªãƒ•ã‚¡ã‚¤ãƒ« |
| åˆ†å‰²æ–¹å¼ | 10 | 55KB | 5KB Ã— 10 | gzipåœ§ç¸®ã§è»½æ¸›å¯èƒ½ |

**ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡**:
- çµåˆæ–¹å¼: 1ã¤ã®`<audio>`è¦ç´ 
- åˆ†å‰²æ–¹å¼: 1ã¤ã®`<audio>`è¦ç´ ï¼ˆsrcã‚’åˆ‡ã‚Šæ›¿ãˆã‚‹ã ã‘ï¼‰
- **çµè«–**: ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã¯ã»ã¼åŒã˜

### APIæ–™é‡‘ã®æ¯”è¼ƒ

**é‡è¦**: æ—¢å­˜å®Ÿè£…ã§ã¯æ—¢ã«æ–‡ã”ã¨ã«ç”Ÿæˆã—ã¦ã„ã‚‹ãŸã‚ã€**æ–™é‡‘ã¯å…¨ãå¢—ãˆã¾ã›ã‚“**ã€‚

---

## ğŸš€ å®Ÿè£…é †åº

### ã‚»ãƒƒã‚·ãƒ§ãƒ³1ï¼ˆ1-1.5æ™‚é–“ï¼‰: ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰å®Ÿè£…

1. `openai_service.py`ã«`generate_speech_separated()`è¿½åŠ ï¼ˆ20åˆ†ï¼‰
2. `tts.py`ã«`/tts-with-timings-separated`ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆè¿½åŠ ï¼ˆ15åˆ†ï¼‰
3. `schemas/tts.py`ã«ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚¹ã‚­ãƒ¼ãƒè¿½åŠ ï¼ˆ5åˆ†ï¼‰
4. ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ãƒ†ã‚¹ãƒˆï¼ˆcurlã§å‹•ä½œç¢ºèªï¼‰ï¼ˆ15åˆ†ï¼‰

### ã‚»ãƒƒã‚·ãƒ§ãƒ³2ï¼ˆ1.5æ™‚é–“ï¼‰: ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰å®Ÿè£…ï¼ˆåŸºæœ¬ï¼‰

1. `api/tts.ts`ã«`performTTSSeparated()`è¿½åŠ ï¼ˆ15åˆ†ï¼‰
2. `constants/api.ts`ã«ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆè¿½åŠ ï¼ˆ5åˆ†ï¼‰
3. `App.tsx`ã®ä¿®æ­£ï¼ˆseparated modeå¯¾å¿œï¼‰ï¼ˆ20åˆ†ï¼‰
4. `AudioPlayer.tsx`ã®åŸºæœ¬å®Ÿè£…ï¼ˆstateã€åˆæœŸåŒ–ã€æ–‡åˆ‡ã‚Šæ›¿ãˆï¼‰ï¼ˆ50åˆ†ï¼‰

### ã‚»ãƒƒã‚·ãƒ§ãƒ³3ï¼ˆ1æ™‚é–“ï¼‰: ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰å®Ÿè£…ï¼ˆå®Œæˆï¼‰

1. `AudioPlayer.tsx`ã®ãƒªãƒ”ãƒ¼ãƒˆæ©Ÿèƒ½å®Ÿè£…ï¼ˆ20åˆ†ï¼‰
2. `AudioPlayer.tsx`ã®ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³æ©Ÿèƒ½å®Ÿè£…ï¼ˆ15åˆ†ï¼‰
3. `AudioPlayer.tsx`ã®ãƒãƒ¼ã‚ºæ©Ÿèƒ½å®Ÿè£…ï¼ˆ15åˆ†ï¼‰
4. UIã®èª¿æ•´ï¼ˆã‚·ãƒ¼ã‚¯ãƒãƒ¼ç°¡ç•¥åŒ–ã€æ–‡ãƒªã‚¹ãƒˆé€£æºï¼‰ï¼ˆ10åˆ†ï¼‰

### ã‚»ãƒƒã‚·ãƒ§ãƒ³4ï¼ˆ30åˆ†ï¼‰: çµ±åˆãƒ†ã‚¹ãƒˆ

1. ãƒ‡ã‚¹ã‚¯ãƒˆãƒƒãƒ—ã§E2Eãƒ†ã‚¹ãƒˆï¼ˆ15åˆ†ï¼‰
2. ãƒ¢ãƒã‚¤ãƒ«ã§E2Eãƒ†ã‚¹ãƒˆï¼ˆ15åˆ†ï¼‰
3. ãƒã‚°ä¿®æ­£ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰

---

## ğŸ”„ äº’æ›æ€§ã¨ç§»è¡Œæˆ¦ç•¥

### æ®µéšçš„ç§»è¡Œ

**Phase A: ä¸¡æ–¹å¼ã‚’ã‚µãƒãƒ¼ãƒˆ**ï¼ˆç¾ãƒ•ã‚§ãƒ¼ã‚ºï¼‰
- æ—¢å­˜ã®`/api/tts-with-timings`ï¼ˆçµåˆæ–¹å¼ï¼‰ã‚’æ®‹ã™
- æ–°ã—ã„`/api/tts-with-timings-separated`ï¼ˆåˆ†å‰²æ–¹å¼ï¼‰ã‚’è¿½åŠ 
- ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã§åˆ‡ã‚Šæ›¿ãˆå¯èƒ½ã«ã™ã‚‹

**Phase B: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚’åˆ†å‰²æ–¹å¼ã«å¤‰æ›´**
- `App.tsx`ã§`performTTSSeparated()`ã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã«
- å•é¡ŒãŒã‚ã‚Œã°`performTTSWithTimings()`ã«æˆ»ã›ã‚‹

**Phase C: çµåˆæ–¹å¼ã‚’å‰Šé™¤**ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
- åˆ†å‰²æ–¹å¼ãŒå®‰å®šã—ãŸã‚‰ã€çµåˆæ–¹å¼ã®ã‚³ãƒ¼ãƒ‰ã‚’å‰Šé™¤

### ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ãƒ•ãƒ©ã‚°ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰

```typescript
// App.tsx
const USE_SEPARATED_AUDIO = true // Toggle between modes

const handleGenerateAudio = async () => {
  if (USE_SEPARATED_AUDIO) {
    // Use separated mode
    const { audioBlobs, durations } = await performTTSSeparated(...)
    setAudioSegments(audioBlobs)
  } else {
    // Use combined mode (legacy)
    const { audioBlob, timings } = await performTTSWithTimings(...)
    setAudioUrl(URL.createObjectURL(audioBlob))
  }
}
```

---

## ğŸ› æ—¢çŸ¥ã®åˆ¶ç´„ã¨å¯¾ç­–

### åˆ¶ç´„1: ãƒ–ãƒ©ã‚¦ã‚¶ã®ä¸¦è¡ŒéŸ³å£°èª­ã¿è¾¼ã¿åˆ¶é™

**å•é¡Œ**: 10å€‹ã®mp3ã‚’ä¸€åº¦ã«èª­ã¿è¾¼ã‚€ã¨ã€ãƒ–ãƒ©ã‚¦ã‚¶ã®ä¸¦è¡Œãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°åˆ¶é™ã«å¼•ã£ã‹ã‹ã‚‹å¯èƒ½æ€§

**å¯¾ç­–**:
- ãƒ—ãƒªãƒ­ãƒ¼ãƒ‰æˆ¦ç•¥: æ¬¡ã®æ–‡ã®ã¿ã‚’äº‹å‰èª­ã¿è¾¼ã¿
- ç¾åœ¨ã®æ–‡ + æ¬¡ã®1-2æ–‡ã‚’ãƒãƒƒãƒ•ã‚¡ãƒªãƒ³ã‚°

```typescript
// Preload next segment
useEffect(() => {
  if (currentSegmentIndex < audioSegments.length - 1) {
    const nextAudio = new Audio(audioSegments[currentSegmentIndex + 1])
    nextAudio.preload = 'auto'
  }
}, [currentSegmentIndex, audioSegments])
```

### åˆ¶ç´„2: æ–‡åˆ‡ã‚Šæ›¿ãˆæ™‚ã®ä¸€ç¬ã®ç„¡éŸ³

**å•é¡Œ**: `audio.src`ã‚’å¤‰æ›´ã™ã‚‹ã¨ã€ä¸€ç¬ã®ç„¡éŸ³ãŒç™ºç”Ÿã™ã‚‹å¯èƒ½æ€§

**å¯¾ç­–**:
- `audio.load()`ã®é«˜é€ŸåŒ–ï¼ˆãƒ—ãƒªãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ãªã‚‰å³åº§ã«å†ç”Ÿï¼‰
- ç„¡éŸ³æœŸé–“ã‚’0.05ç§’æœªæº€ã«æŠ‘ãˆã‚‹

**æ¤œè¨¼**: å®Ÿè£…å¾Œã€å®Ÿéš›ã®ç„¡éŸ³æœŸé–“ã‚’æ¸¬å®š

---

## âœ… æˆåŠŸåŸºæº–

### å¿…é ˆè¦ä»¶

1. **ãƒªãƒ”ãƒ¼ãƒˆæ©Ÿèƒ½ãŒå®Œç’§ã«å‹•ä½œ**
   - 3å›è¨­å®šã§ç¢ºå®Ÿã«3å›å†ç”Ÿã•ã‚Œã‚‹
   - ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼è¡¨ç¤ºãŒæ­£ç¢º

2. **æ–‡ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ãŒå³åº§ã«åå¿œ**
   - ã€Œæ¬¡ã®æ–‡ã€ãƒœã‚¿ãƒ³ã§å³åº§ã«ç§»å‹•
   - ã‚·ãƒ¼ã‚¯æ“ä½œãªã—ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«åˆ‡ã‚Šæ›¿ãˆã®ã¿ï¼‰

3. **ãƒãƒ¼ã‚ºå‰ã®éŸ³è¢«ã‚Šå•é¡ŒãŒå®Œå…¨è§£æ±º**
   - ãƒãƒ¼ã‚ºæ™‚ã«æ¬¡ã®æ–‡ã®éŸ³ãŒèã“ãˆãªã„

4. **APIæ–™é‡‘ãŒå¢—ãˆãªã„**
   - æ—¢å­˜ã¨åŒã˜APIå‘¼ã³å‡ºã—å›æ•°

### ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¦ä»¶

1. **æ–‡åˆ‡ã‚Šæ›¿ãˆé€Ÿåº¦ < 0.1ç§’**
   - ãƒ—ãƒªãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ã®å ´åˆ

2. **åˆå›èª­ã¿è¾¼ã¿æ™‚é–“ < 5ç§’**
   - 10æ–‡ã®å ´åˆ

3. **ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ < æ—¢å­˜ã®1.2å€**

---

## ğŸ“ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

### ã“ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã§å®Ÿæ–½

1. âœ… è¨­è¨ˆæ›¸ä½œæˆï¼ˆå®Œäº†ï¼‰
2. ğŸ”„ ãƒ¦ãƒ¼ã‚¶ãƒ¼æ‰¿èªå¾…ã¡

### æ¬¡å›ã‚»ãƒƒã‚·ãƒ§ãƒ³ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼æ‰¿èªå¾Œï¼‰

1. ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰å®Ÿè£…ï¼ˆ30-45åˆ†ï¼‰
2. ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ãƒ†ã‚¹ãƒˆï¼ˆ15åˆ†ï¼‰
3. ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰å®Ÿè£…é–‹å§‹ï¼ˆ1æ™‚é–“ï¼‰

---

## ğŸ”— å‚è€ƒãƒ•ã‚¡ã‚¤ãƒ«

**ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰**:
- `backend/app/services/openai_service.py` (lines 173-329)
- `backend/app/api/routes/tts.py` (lines 98-190)
- `backend/app/schemas/tts.py`

**ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰**:
- `frontend/src/services/api/tts.ts`
- `frontend/src/components/features/AudioPlayer/AudioPlayer.tsx`
- `frontend/src/App.tsx`

**ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ**:
- `docs/sessions/HANDOVER.md` (Session #17)
- `docs/sessions/TODO.md`
